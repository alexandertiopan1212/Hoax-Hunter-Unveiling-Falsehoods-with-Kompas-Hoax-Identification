{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoax Identification from Kompas News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project \"Hoax Identification from Kompas News Modeling\" aims to develop a system that can automatically identify and classify hoax or fake news articles from the Kompas news website. This project is significant due to the rise of misinformation and fake news circulating on social media and other platforms, leading to potential harm to individuals and society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-03-10 00:19:10.483301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets, fact_df and hoax_df, are imported from Excel files. fact_df contains cleaned factual information labeled as 0 from dataset_kompas_4k_cleaned.xlsx, while hoax_df contains cleaned hoax information labeled as 1 from dataset_turnbackhoax_10_cleaned.xlsx. The datasets likely refer to the number of records (4,000 and 10,000, respectively) and have been cleaned, suggesting removal of irrelevant or erroneous data and standardization for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Concat Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 11581,
     "status": "ok",
     "timestamp": 1698733065295,
     "user": {
      "displayName": "pycodes",
      "userId": "00703128687656036027"
     },
     "user_tz": -420
    },
    "id": "ZHJLOlHHAAhl",
    "outputId": "6c48c9cf-18c9-43f6-ca65-ec5ca0491a85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0\n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0\n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0\n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0\n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_df = pd.read_excel('./dataset_kompas_4k_cleaned.xlsx')\n",
    "fact_df['Label'] = 0\n",
    "\n",
    "hoax_df = pd.read_excel('./dataset_turnbackhoax_10_cleaned.xlsx')\n",
    "hoax_df['Label'] = 1\n",
    "\n",
    "news_df = pd.concat([fact_df, hoax_df], axis=0, ignore_index=True)\n",
    "news_df = news_df[['FullText', 'Label']]\n",
    "\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15131 entries, 0 to 15130\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   FullText  15104 non-null  object\n",
      " 1   Label     15131 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 236.6+ KB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15082 entries, 0 to 15130\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   FullText  15082 non-null  object\n",
      " 1   Label     15082 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 353.5+ KB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df['LowerCase'] = news_df['FullText'].apply(lambda text: lowercase_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "      <th>LowerCase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - pemerintah menargetkan p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>pdi-perjuangan, partai gerindra, dan partai go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - survei litbang kompas ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - presiden joko widodo la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label  \\\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0   \n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0   \n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0   \n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0   \n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0   \n",
       "\n",
       "                                           LowerCase  \n",
       "0  hasil jajak pendapat yang diselenggarakan litb...  \n",
       "1  jakarta, kompas.com - pemerintah menargetkan p...  \n",
       "2  pdi-perjuangan, partai gerindra, dan partai go...  \n",
       "3  jakarta, kompas.com - survei litbang kompas ja...  \n",
       "4   jakarta, kompas.com - presiden joko widodo la...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unnecessary Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace '-' with space to prevent repeated words and hyphens between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a series of text preprocessing functions to the 'LowerCase' column of a DataFrame news_df, sequentially removing multiple types of unnecessary characters such as punctuation, special characters, single characters, digits, ASCII characters, Unicode characters, newlines, and extra spaces. The result is stored in a new column called 'RemoveUnnecessaryCharacters', preparing the text for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strip(text):\n",
    "    return text.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(text):\n",
    "    return re.sub(r'\\W', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_character(text):\n",
    "    return re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digit_number(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ascii(text):\n",
    "    return text.encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline_etc(text):\n",
    "    return text.replace('\\\\t',\"\").replace('\\\\n',\"\").replace('\\\\u',\" \").replace('\\\\',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multispace(text):\n",
    "    return re.sub('\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['RemoveUnnecessaryCharacters'] = news_df['LowerCase'].apply(lambda text: remove_punctuation(\n",
    "    remove_strip(\n",
    "        remove_special_character(\n",
    "            remove_single_character(\n",
    "                remove_digit_number(\n",
    "                    remove_ascii(\n",
    "                        remove_unicode(\n",
    "                            remove_newline_etc(\n",
    "                                remove_multispace(text)\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "      <th>LowerCase</th>\n",
       "      <th>RemoveUnnecessaryCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - pemerintah menargetkan p...</td>\n",
       "      <td>jakarta  kompas com   pemerintah menargetkan p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>pdi-perjuangan, partai gerindra, dan partai go...</td>\n",
       "      <td>pdi perjuangan  partai gerindra  dan partai go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - survei litbang kompas ja...</td>\n",
       "      <td>jakarta  kompas com   survei litbang kompas ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - presiden joko widodo la...</td>\n",
       "      <td>jakarta  kompas com   presiden joko widodo la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label  \\\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0   \n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0   \n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0   \n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0   \n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0   \n",
       "\n",
       "                                           LowerCase  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta, kompas.com - pemerintah menargetkan p...   \n",
       "2  pdi-perjuangan, partai gerindra, dan partai go...   \n",
       "3  jakarta, kompas.com - survei litbang kompas ja...   \n",
       "4   jakarta, kompas.com - presiden joko widodo la...   \n",
       "\n",
       "                         RemoveUnnecessaryCharacters  \n",
       "0  hasil jajak pendapat yang diselenggarakan litb...  \n",
       "1  jakarta  kompas com   pemerintah menargetkan p...  \n",
       "2  pdi perjuangan  partai gerindra  dan partai go...  \n",
       "3  jakarta  kompas com   survei litbang kompas ja...  \n",
       "4   jakarta  kompas com   presiden joko widodo la...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1063KCaUPwu"
   },
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a CSV file containing Indonesian stopwords and renames the first column to 'stopword'. It defines a function remove_stopword to remove stopwords from text, applying it to a DataFrame column called 'RemoveUnnecessaryCharacters' and storing the result in a new column called 'RemoveStopword'. The function iterates over each word in the text, replacing stopwords with an empty string, and then removes extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_stopword_dict = pd.read_csv('./stopwordbahasa.csv', header=None)\n",
    "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adalah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adanya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adapun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0      ada\n",
       "1   adalah\n",
       "2   adanya\n",
       "3   adapun\n",
       "4     agak"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_stopword_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['RemoveStopword'] = news_df['RemoveUnnecessaryCharacters'].apply(lambda text: remove_stopword(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "      <th>LowerCase</th>\n",
       "      <th>RemoveUnnecessaryCharacters</th>\n",
       "      <th>RemoveStopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat diselenggarakan litbang k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - pemerintah menargetkan p...</td>\n",
       "      <td>jakarta  kompas com   pemerintah menargetkan p...</td>\n",
       "      <td>jakarta kompas com pemerintah menargetkan pert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>pdi-perjuangan, partai gerindra, dan partai go...</td>\n",
       "      <td>pdi perjuangan  partai gerindra  dan partai go...</td>\n",
       "      <td>pdi perjuangan partai gerindra partai golkar m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - survei litbang kompas ja...</td>\n",
       "      <td>jakarta  kompas com   survei litbang kompas ja...</td>\n",
       "      <td>jakarta kompas com survei litbang kompas janua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - presiden joko widodo la...</td>\n",
       "      <td>jakarta  kompas com   presiden joko widodo la...</td>\n",
       "      <td>jakarta kompas com presiden joko widodo bicara...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label  \\\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0   \n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0   \n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0   \n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0   \n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0   \n",
       "\n",
       "                                           LowerCase  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta, kompas.com - pemerintah menargetkan p...   \n",
       "2  pdi-perjuangan, partai gerindra, dan partai go...   \n",
       "3  jakarta, kompas.com - survei litbang kompas ja...   \n",
       "4   jakarta, kompas.com - presiden joko widodo la...   \n",
       "\n",
       "                         RemoveUnnecessaryCharacters  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta  kompas com   pemerintah menargetkan p...   \n",
       "2  pdi perjuangan  partai gerindra  dan partai go...   \n",
       "3  jakarta  kompas com   survei litbang kompas ja...   \n",
       "4   jakarta  kompas com   presiden joko widodo la...   \n",
       "\n",
       "                                      RemoveStopword  \n",
       "0  hasil jajak pendapat diselenggarakan litbang k...  \n",
       "1  jakarta kompas com pemerintah menargetkan pert...  \n",
       "2  pdi perjuangan partai gerindra partai golkar m...  \n",
       "3  jakarta kompas com survei litbang kompas janua...  \n",
       "4  jakarta kompas com presiden joko widodo bicara...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the Sastrawi library for stemming in Bahasa Indonesia. It applies the stemming function to the 'RemoveStopword' column of a DataFrame news_df, which contains text data after stopwords have been removed. The stemming process reduces each word to its base or root form, which can help in text analysis by grouping together variations of words with the same meaning. The result is stored in a new column called 'Stemming', which can be used for further analysis or processing of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PySastrawi in /Applications/anaconda/anaconda3/lib/python3.11/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PySastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    return stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['Stemming'] = news_df['RemoveStopword'].apply(lambda text: stemming(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "      <th>LowerCase</th>\n",
       "      <th>RemoveUnnecessaryCharacters</th>\n",
       "      <th>RemoveStopword</th>\n",
       "      <th>Stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat diselenggarakan litbang k...</td>\n",
       "      <td>hasil jajak dapat selenggara litbang kompas ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - pemerintah menargetkan p...</td>\n",
       "      <td>jakarta  kompas com   pemerintah menargetkan p...</td>\n",
       "      <td>jakarta kompas com pemerintah menargetkan pert...</td>\n",
       "      <td>jakarta kompas com perintah target tumbuh ekon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>pdi-perjuangan, partai gerindra, dan partai go...</td>\n",
       "      <td>pdi perjuangan  partai gerindra  dan partai go...</td>\n",
       "      <td>pdi perjuangan partai gerindra partai golkar m...</td>\n",
       "      <td>pdi juang partai gerindra partai golkar tempat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - survei litbang kompas ja...</td>\n",
       "      <td>jakarta  kompas com   survei litbang kompas ja...</td>\n",
       "      <td>jakarta kompas com survei litbang kompas janua...</td>\n",
       "      <td>jakarta kompas com survei litbang kompas janua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - presiden joko widodo la...</td>\n",
       "      <td>jakarta  kompas com   presiden joko widodo la...</td>\n",
       "      <td>jakarta kompas com presiden joko widodo bicara...</td>\n",
       "      <td>jakarta kompas com presiden joko widodo bicara...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label  \\\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0   \n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0   \n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0   \n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0   \n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0   \n",
       "\n",
       "                                           LowerCase  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta, kompas.com - pemerintah menargetkan p...   \n",
       "2  pdi-perjuangan, partai gerindra, dan partai go...   \n",
       "3  jakarta, kompas.com - survei litbang kompas ja...   \n",
       "4   jakarta, kompas.com - presiden joko widodo la...   \n",
       "\n",
       "                         RemoveUnnecessaryCharacters  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta  kompas com   pemerintah menargetkan p...   \n",
       "2  pdi perjuangan  partai gerindra  dan partai go...   \n",
       "3  jakarta  kompas com   survei litbang kompas ja...   \n",
       "4   jakarta  kompas com   presiden joko widodo la...   \n",
       "\n",
       "                                      RemoveStopword  \\\n",
       "0  hasil jajak pendapat diselenggarakan litbang k...   \n",
       "1  jakarta kompas com pemerintah menargetkan pert...   \n",
       "2  pdi perjuangan partai gerindra partai golkar m...   \n",
       "3  jakarta kompas com survei litbang kompas janua...   \n",
       "4  jakarta kompas com presiden joko widodo bicara...   \n",
       "\n",
       "                                            Stemming  \n",
       "0  hasil jajak dapat selenggara litbang kompas ja...  \n",
       "1  jakarta kompas com perintah target tumbuh ekon...  \n",
       "2  pdi juang partai gerindra partai golkar tempat...  \n",
       "3  jakarta kompas com survei litbang kompas janua...  \n",
       "4  jakarta kompas com presiden joko widodo bicara...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text in the 'Stemming' column of a DataFrame news_df, splitting it into smaller units like words or sentences. The apply method is used to apply tokenize_text to each element in 'Stemming', and the result is stored in a new column 'word_tokenize', where each row contains a list of tokens extracted from the corresponding text. This tokenization step prepares the text data for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['word_tokenize'] = news_df['Stemming'].apply(lambda text: tokenize_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullText</th>\n",
       "      <th>Label</th>\n",
       "      <th>LowerCase</th>\n",
       "      <th>RemoveUnnecessaryCharacters</th>\n",
       "      <th>RemoveStopword</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>word_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasil jajak pendapat yang diselenggarakan Litb...</td>\n",
       "      <td>0</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat yang diselenggarakan litb...</td>\n",
       "      <td>hasil jajak pendapat diselenggarakan litbang k...</td>\n",
       "      <td>hasil jajak dapat selenggara litbang kompas ja...</td>\n",
       "      <td>[hasil, jajak, dapat, selenggara, litbang, kom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Pemerintah menargetkan p...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - pemerintah menargetkan p...</td>\n",
       "      <td>jakarta  kompas com   pemerintah menargetkan p...</td>\n",
       "      <td>jakarta kompas com pemerintah menargetkan pert...</td>\n",
       "      <td>jakarta kompas com perintah target tumbuh ekon...</td>\n",
       "      <td>[jakarta, kompas, com, perintah, target, tumbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDI-Perjuangan, Partai Gerindra, dan Partai Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>pdi-perjuangan, partai gerindra, dan partai go...</td>\n",
       "      <td>pdi perjuangan  partai gerindra  dan partai go...</td>\n",
       "      <td>pdi perjuangan partai gerindra partai golkar m...</td>\n",
       "      <td>pdi juang partai gerindra partai golkar tempat...</td>\n",
       "      <td>[pdi, juang, partai, gerindra, partai, golkar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - survei litbang kompas ja...</td>\n",
       "      <td>jakarta  kompas com   survei litbang kompas ja...</td>\n",
       "      <td>jakarta kompas com survei litbang kompas janua...</td>\n",
       "      <td>jakarta kompas com survei litbang kompas janua...</td>\n",
       "      <td>[jakarta, kompas, com, survei, litbang, kompas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Joko Widodo la...</td>\n",
       "      <td>0</td>\n",
       "      <td>jakarta, kompas.com - presiden joko widodo la...</td>\n",
       "      <td>jakarta  kompas com   presiden joko widodo la...</td>\n",
       "      <td>jakarta kompas com presiden joko widodo bicara...</td>\n",
       "      <td>jakarta kompas com presiden joko widodo bicara...</td>\n",
       "      <td>[jakarta, kompas, com, presiden, joko, widodo,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FullText  Label  \\\n",
       "0  Hasil jajak pendapat yang diselenggarakan Litb...      0   \n",
       "1  JAKARTA, KOMPAS.com - Pemerintah menargetkan p...      0   \n",
       "2  PDI-Perjuangan, Partai Gerindra, dan Partai Go...      0   \n",
       "3  JAKARTA, KOMPAS.com - Survei Litbang Kompas Ja...      0   \n",
       "4   JAKARTA, KOMPAS.com - Presiden Joko Widodo la...      0   \n",
       "\n",
       "                                           LowerCase  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta, kompas.com - pemerintah menargetkan p...   \n",
       "2  pdi-perjuangan, partai gerindra, dan partai go...   \n",
       "3  jakarta, kompas.com - survei litbang kompas ja...   \n",
       "4   jakarta, kompas.com - presiden joko widodo la...   \n",
       "\n",
       "                         RemoveUnnecessaryCharacters  \\\n",
       "0  hasil jajak pendapat yang diselenggarakan litb...   \n",
       "1  jakarta  kompas com   pemerintah menargetkan p...   \n",
       "2  pdi perjuangan  partai gerindra  dan partai go...   \n",
       "3  jakarta  kompas com   survei litbang kompas ja...   \n",
       "4   jakarta  kompas com   presiden joko widodo la...   \n",
       "\n",
       "                                      RemoveStopword  \\\n",
       "0  hasil jajak pendapat diselenggarakan litbang k...   \n",
       "1  jakarta kompas com pemerintah menargetkan pert...   \n",
       "2  pdi perjuangan partai gerindra partai golkar m...   \n",
       "3  jakarta kompas com survei litbang kompas janua...   \n",
       "4  jakarta kompas com presiden joko widodo bicara...   \n",
       "\n",
       "                                            Stemming  \\\n",
       "0  hasil jajak dapat selenggara litbang kompas ja...   \n",
       "1  jakarta kompas com perintah target tumbuh ekon...   \n",
       "2  pdi juang partai gerindra partai golkar tempat...   \n",
       "3  jakarta kompas com survei litbang kompas janua...   \n",
       "4  jakarta kompas com presiden joko widodo bicara...   \n",
       "\n",
       "                                       word_tokenize  \n",
       "0  [hasil, jajak, dapat, selenggara, litbang, kom...  \n",
       "1  [jakarta, kompas, com, perintah, target, tumbu...  \n",
       "2  [pdi, juang, partai, gerindra, partai, golkar,...  \n",
       "3  [jakarta, kompas, com, survei, litbang, kompas...  \n",
       "4  [jakarta, kompas, com, presiden, joko, widodo,...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer UNIGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's CountVectorizer to convert a list of tokenized news articles stored in 'word_tokenize' column of a DataFrame news_df into a matrix of token counts. The map function is used to join the tokens back into sentences. The fit_transform method of CountVectorizer converts the list of news articles into a matrix X where each row represents a news article and each column represents a unique token. Finally, get_feature_names_out is used to get the list of unique tokens, and X.toarray() is used to print the matrix of token counts for each news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "news_list = news_df['word_tokenize'].map(' '.join)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(news_list)\n",
    "vectorizer.get_feature_names_out()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, news_df['Label'], test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9967796931236976\n",
      "precision: 0.9963933971424608\n",
      "recall: 0.9988874982617161\n",
      "f1-measure: 0.997638888888889\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"precision:\", precision)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"recall:\", recall)\n",
    "\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "print(\"f1-measure:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest evaluation model demonstrates high performance across multiple metrics, including accuracy (0.997), precision (0.996), recall (0.999), and F1-measure (0.998). These scores indicate that the model is very accurate in its predictions, with a high ability to correctly identify positive instances and avoid false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert X to Sparse Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a sparse matrix X into a sparse tensor X_sparse_tensor using TensorFlow. First, X is converted to COO (Coordinate Format) using tocoo(). Then, the row and column indices are stacked together to create a 2D array of indices. Finally, a sparse tensor X_sparse_tensor is created using the SparseTensor class from TensorFlow, with the indices, data (values), and shape of the COO matrix. This conversion allows the sparse matrix X to be used efficiently in TensorFlow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_coo = X.tocoo()\n",
    "indices = np.column_stack((X_coo.row, X_coo.col))\n",
    "X_sparse_tensor = tf.SparseTensor(indices, X_coo.data, X_coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[    0 38534]\n",
      " [    0 46181]\n",
      " [    0 21496]\n",
      " ...\n",
      " [15081 78656]\n",
      " [15081 30553]\n",
      " [15081 76753]], shape=(1891203, 2), dtype=int64), values=tf.Tensor([1 2 1 ... 1 1 2], shape=(1891203,), dtype=int64), dense_shape=tf.Tensor([ 15082 118576], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(X_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert The SparseTensor to Dense NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ordered_sparse_tensor = tf.sparse.reorder(X_sparse_tensor)\n",
    "X_dense = tf.sparse.to_dense(X_ordered_sparse_tensor).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(news_df['Label'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape The Input Data to Include The Sequence Length Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_dense.shape[1]  # The length of your vocabulary\n",
    "X_dense = X_dense.reshape(-1, sequence_length, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cast The Input Data to The Appropriate Data Type (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense = X_dense.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a convolutional neural network (CNN) model using Keras' Sequential API. The model consists of a 1D convolutional layer with 64 filters and a kernel size of 3, followed by a global max pooling layer to reduce the dimensionality of the features. Finally, a dense layer with the number of units equal to the number of classes and a softmax activation function is added to output the probability distribution over the classes. This type of architecture is commonly used for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 10s/step - accuracy: 0.4502 - loss: 2.0578 - val_accuracy: 0.6811 - val_loss: 0.6144\n",
      "Epoch 2/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 9s/step - accuracy: 0.6976 - loss: 0.5992 - val_accuracy: 0.6811 - val_loss: 0.6084\n",
      "Epoch 3/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 9s/step - accuracy: 0.6911 - loss: 0.6027 - val_accuracy: 0.6811 - val_loss: 0.6081\n",
      "Epoch 4/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 9s/step - accuracy: 0.6915 - loss: 0.6062 - val_accuracy: 0.6811 - val_loss: 0.6086\n",
      "Epoch 5/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 9s/step - accuracy: 0.7033 - loss: 0.5846 - val_accuracy: 0.6811 - val_loss: 0.6044\n",
      "Epoch 6/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 9s/step - accuracy: 0.7031 - loss: 0.5923 - val_accuracy: 0.6811 - val_loss: 0.6045\n",
      "Epoch 7/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 9s/step - accuracy: 0.6950 - loss: 0.5877 - val_accuracy: 0.6811 - val_loss: 0.6043\n",
      "Epoch 8/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 9s/step - accuracy: 0.6951 - loss: 0.5955 - val_accuracy: 0.6811 - val_loss: 0.6052\n",
      "Epoch 9/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 9s/step - accuracy: 0.6907 - loss: 0.5972 - val_accuracy: 0.6811 - val_loss: 0.6038\n",
      "Epoch 10/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 9s/step - accuracy: 0.6996 - loss: 0.5939 - val_accuracy: 0.6811 - val_loss: 0.6036\n",
      "Epoch 11/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 9s/step - accuracy: 0.7081 - loss: 0.5941 - val_accuracy: 0.6811 - val_loss: 0.6036\n",
      "Epoch 12/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 9s/step - accuracy: 0.6973 - loss: 0.5905 - val_accuracy: 0.6811 - val_loss: 0.6025\n",
      "Epoch 13/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 9s/step - accuracy: 0.6976 - loss: 0.5943 - val_accuracy: 0.6811 - val_loss: 0.6033\n",
      "Epoch 14/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7028 - loss: 0.5837"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 931ms/step - accuracy: 0.6801 - loss: 0.6048\n",
      "Test Loss: 0.6018175482749939, Test Accuracy: 0.6810948848724365\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6810948848724365\n",
      "precision: 0.46389026871511146\n",
      "recall: 0.6810949043379428\n",
      "loss: 0.6018175482749939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
    "print(\"precision:\", precision)\n",
    "\n",
    "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
    "print(\"recall:\", recall)\n",
    "\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN model evaluation indicates moderate performance. The accuracy of 0.681 suggests that about 68% of the predictions were correct. The precision score of 0.464 indicates that when the model predicts a positive class, it is correct about 46% of the time. The recall score of 0.681 indicates that the model correctly identifies about 68% of all actual positive instances. The loss value of 0.602 is a measure of the model's error, with lower values indicating better performance. Overall, while the model shows decent accuracy and recall, there is room for improvement in precision, as it is relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest vs CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the evaluation models between random forest and a convolutional neural network (CNN) for detecting hoaxes using the news_df dataset, we need to consider the nature of the dataset and the characteristics of each model:\n",
    "\n",
    "* Dataset Characteristics: The news_df dataset likely contains textual data, such as news articles or headlines, which can be complex and contain various linguistic nuances. Textual data requires careful preprocessing to extract meaningful features for classification tasks.\n",
    "\n",
    "* Random Forest: Random forests are robust and perform well with high-dimensional datasets like text data. They can handle a large number of input features and are less prone to overfitting, making them suitable for text classification tasks. Random forests can capture complex relationships between features and are interpretable, providing insights into which features are important for classification.\n",
    "\n",
    "* CNN: CNNs are effective for tasks involving spatial relationships, such as image recognition, but can also be applied to sequential data like text. CNNs can automatically learn relevant features from the data, which is beneficial for text classification. However, they require a large amount of data to generalize well and may be prone to overfitting, especially with small datasets.\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "* Accuracy: Random forests tend to perform well on text classification tasks, achieving high accuracy by leveraging the diversity of decision trees. However, CNNs can potentially achieve higher accuracy by learning intricate patterns in text data.\n",
    "* Precision and Recall: Random forests often exhibit high precision and recall due to their ability to handle imbalanced datasets well. CNNs may struggle with imbalanced datasets but can perform comparably with proper tuning.\n",
    "* Interpretability: Random forests are more interpretable than CNNs, as they provide feature importances for each input feature. This can be useful for understanding which words or features are most important for hoax detection.\n",
    "* Computational Complexity: CNNs are computationally more expensive than random forests, especially for training on large datasets. Random forests are generally faster to train and can be more efficient for smaller datasets.\n",
    "\n",
    "\n",
    "In conclusion, while random forests may perform well and provide interpretability for hoax detection on the news_df dataset, CNNs have the potential to achieve higher accuracy by automatically learning intricate patterns in textual data. However, CNNs may require more computational resources and careful tuning to avoid overfitting, especially with smaller datasets like news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
